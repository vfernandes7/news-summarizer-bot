import requests
from bs4 import BeautifulSoup
from datetime import datetime
import json
import openai
from dotenv import load_dotenv, find_dotenv
import os
import time
from pathlib import Path


# Getting G1 Politics News HTML and creating a BeautifulSoup object
with requests.session() as s:
    url = 'https://g1.globo.com/politica/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                      '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
    }
    req = s.get(url, headers=headers)
soup = BeautifulSoup(req.content, 'html.parser')

# Extracting the main news and saving on a list
news_part1 = soup.select('._evt .bastian-feed-item h2 a')
# Extracting older news that loads from a javascript after scroll
news_part2 = json.loads(soup.find_all('script', id='bstn-launcher-bundle')[0].get_text().split('"items":')[1].split(']')[0]+']')


# Searching for the URL and title of each news on both lists and saving in a dictionary
news_list = []

for news_info in news_part1:
    news_dict = {}
    news_dict['url'] = news_info['href']
    news_dict['title'] = news_info.text.strip()
    news_list.append(news_dict)
    
for news_info in news_part2:
    news_dict = {}
    news_dict['url'] = news_info['content']['url']
    news_dict['title'] = news_info['content']['title'].strip()
    news_list.append(news_dict)

#removing news that are only video and news that has #FAKE on its title
news_list = [news for news in news_list if not '/video/' in news['url']]
news_list = [news for news in news_list if not '#FAKE' in news['title']]


#selecting the latest 15 news and extracting its content, and saving on the dictionary
news_for_summary = 15
for news in news_list[:news_for_summary]:

    print(news['title'])

    with requests.session() as s:
        news_url = news['url']
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                        '(KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
        }
        news_req = s.get(news_url, headers=headers)
    news_soup = BeautifulSoup(news_req.content, 'html.parser')
    news_content = "\n".join([content_box.text.strip() for content_box in news_soup.select('.content-text__container')])
    news['content'] = news_content

# creating a txt file with each news title and its content
all_news = ''
for news in news_list[:news_for_summary]:
    title = news['title']
    content = news['content']
    all_news += (f'''
### {title}
{content}
#######
\n
''')
    
# creating a txt file with each news title and its url
news_source = ''
for news in news_list[:news_for_summary]:
    title = news['title']
    url = news['url']
    news_source += (f'''
*{title}*
{url}
''')

#saving both files
with open('news.txt', 'w', encoding='utf-8') as file:
    file.write(all_news)
with open('source.txt', 'w', encoding='utf-8') as file:
    file.write(news_source)

#creating OpenAI client from project key on .env
load_dotenv()
openai_assistant_id = os.getenv('OPENAI_ASSISTANT_ID')
client = openai.Client()

#creating thread, and adding a message to this thread with the content from the news
thread = client.beta.threads.create()
thread_message = client.beta.threads.messages.create(
  thread_id=thread.id,
  role="user",
  content=all_news,
)

#running the thread and waiting it process
run = client.beta.threads.runs.create(
    thread_id=thread.id,
    assistant_id=openai_assistant_id
)

while run.status in ['queued', 'in_progress', 'cancelling']:
    print(run.status)
    time.sleep(5)
    print('processando mensagem...')
    run = client.beta.threads.runs.retrieve(
        thread_id=thread.id,
        run_id=run.id
    )

#when processing finishes, get the message generated by the AI Assistant 
if run.status == 'completed':
    messages = client.beta.threads.messages.list(
        thread_id=thread.id
    )
    summary = messages.data[0].content[0].text.value
else:
    print('Erro', run.status)

#saving the AI summarized news
with open('summary.txt', 'w', encoding='utf-8') as file:
    file.write(summary)

#generating TTS audio and saving it
response = client.audio.speech.create(
    model="tts-1",
    voice="alloy",
    input=summary,
)
response.write_to_file("summarized_news_audio.mp3")
